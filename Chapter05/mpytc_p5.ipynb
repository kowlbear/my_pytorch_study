{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델링 위한 트랜스포머 모델 만들기!\n",
    "언어 모델링\n",
    "특정 언어 시퀀스가 주어질 때 그 뒤를 따르는 단어나 단어 시퀀스 발생 확률을 알아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer # 트랜스포머 특화 라이브러리\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        msk = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        msk = msk.float().masked_fill(msk == 0, float('-inf'))\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0))\n",
    "        return msk\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source):\n",
    "        if self.mask_source is None or self.mask_source.size(0) != len(source):\n",
    "            dvc = source.device\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc)\n",
    "            self.mask_source = msk\n",
    "\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        # d_m is same as the dimension of the embeddings\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, d_m)\n",
    "        pos = torch.arange(0, size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        # divider is the list of radians, multiplied by position indices of words, and fed to the sinusoidal and cosinusoidal function\n",
    "        p_enc[:, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 1::2] = torch.cos(pos * divider)\n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wikitext-2-v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\wikitext-2\\wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:03<00:00, 1.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].view(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(TEXT.vocab.stoi) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        optim_module.zero_grad()\n",
    "        op = transformer_model(train_data_batch)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            op = eval_model_obj(eval_data)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += len(eval_data) * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/1018 batches, training loss 8.54, training perplexity 5116.52\n",
      "epoch 1, 200/1018 batches, training loss 7.21, training perplexity 1358.01\n",
      "epoch 1, 300/1018 batches, training loss 6.78, training perplexity 882.41\n",
      "epoch 1, 400/1018 batches, training loss 6.56, training perplexity 705.68\n",
      "epoch 1, 500/1018 batches, training loss 6.47, training perplexity 643.14\n",
      "epoch 1, 600/1018 batches, training loss 6.33, training perplexity 559.51\n",
      "epoch 1, 700/1018 batches, training loss 6.25, training perplexity 519.17\n",
      "epoch 1, 800/1018 batches, training loss 6.14, training perplexity 462.04\n",
      "epoch 1, 900/1018 batches, training loss 6.10, training perplexity 447.68\n",
      "epoch 1, 1000/1018 batches, training loss 6.07, training perplexity 433.29\n",
      "\n",
      "epoch 1, validation loss 5.88, validation perplexity 357.43\n",
      "\n",
      "epoch 2, 100/1018 batches, training loss 5.98, training perplexity 397.00\n",
      "epoch 2, 200/1018 batches, training loss 5.91, training perplexity 368.04\n",
      "epoch 2, 300/1018 batches, training loss 5.83, training perplexity 341.37\n",
      "epoch 2, 400/1018 batches, training loss 5.80, training perplexity 329.61\n",
      "epoch 2, 500/1018 batches, training loss 5.82, training perplexity 335.96\n",
      "epoch 2, 600/1018 batches, training loss 5.77, training perplexity 321.42\n",
      "epoch 2, 700/1018 batches, training loss 5.78, training perplexity 324.45\n",
      "epoch 2, 800/1018 batches, training loss 5.65, training perplexity 284.45\n",
      "epoch 2, 900/1018 batches, training loss 5.68, training perplexity 293.17\n",
      "epoch 2, 1000/1018 batches, training loss 5.71, training perplexity 300.84\n",
      "\n",
      "epoch 2, validation loss 5.52, validation perplexity 250.50\n",
      "\n",
      "epoch 3, 100/1018 batches, training loss 5.67, training perplexity 291.08\n",
      "epoch 3, 200/1018 batches, training loss 5.60, training perplexity 270.72\n",
      "epoch 3, 300/1018 batches, training loss 5.55, training perplexity 258.24\n",
      "epoch 3, 400/1018 batches, training loss 5.52, training perplexity 250.85\n",
      "epoch 3, 500/1018 batches, training loss 5.55, training perplexity 256.39\n",
      "epoch 3, 600/1018 batches, training loss 5.53, training perplexity 251.43\n",
      "epoch 3, 700/1018 batches, training loss 5.54, training perplexity 254.75\n",
      "epoch 3, 800/1018 batches, training loss 5.39, training perplexity 219.54\n",
      "epoch 3, 900/1018 batches, training loss 5.45, training perplexity 232.01\n",
      "epoch 3, 1000/1018 batches, training loss 5.49, training perplexity 241.96\n",
      "\n",
      "epoch 3, validation loss 5.41, validation perplexity 223.35\n",
      "\n",
      "epoch 4, 100/1018 batches, training loss 5.47, training perplexity 236.46\n",
      "epoch 4, 200/1018 batches, training loss 5.40, training perplexity 221.83\n",
      "epoch 4, 300/1018 batches, training loss 5.37, training perplexity 214.06\n",
      "epoch 4, 400/1018 batches, training loss 5.35, training perplexity 209.71\n",
      "epoch 4, 500/1018 batches, training loss 5.36, training perplexity 213.74\n",
      "epoch 4, 600/1018 batches, training loss 5.35, training perplexity 210.94\n",
      "epoch 4, 700/1018 batches, training loss 5.37, training perplexity 214.09\n",
      "epoch 4, 800/1018 batches, training loss 5.21, training perplexity 182.98\n",
      "epoch 4, 900/1018 batches, training loss 5.28, training perplexity 195.49\n",
      "epoch 4, 1000/1018 batches, training loss 5.33, training perplexity 205.48\n",
      "\n",
      "epoch 4, validation loss 5.30, validation perplexity 200.40\n",
      "\n",
      "epoch 5, 100/1018 batches, training loss 5.31, training perplexity 202.44\n",
      "epoch 5, 200/1018 batches, training loss 5.25, training perplexity 190.14\n",
      "epoch 5, 300/1018 batches, training loss 5.22, training perplexity 185.61\n",
      "epoch 5, 400/1018 batches, training loss 5.20, training perplexity 181.49\n",
      "epoch 5, 500/1018 batches, training loss 5.22, training perplexity 185.08\n",
      "epoch 5, 600/1018 batches, training loss 5.21, training perplexity 183.26\n",
      "epoch 5, 700/1018 batches, training loss 5.23, training perplexity 185.95\n",
      "epoch 5, 800/1018 batches, training loss 5.07, training perplexity 159.13\n",
      "epoch 5, 900/1018 batches, training loss 5.13, training perplexity 169.03\n",
      "epoch 5, 1000/1018 batches, training loss 5.19, training perplexity 179.16\n",
      "\n",
      "epoch 5, validation loss 5.24, validation perplexity 188.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 5\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 5.15, testing perplexity 171.81\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpytc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
